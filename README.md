# 10 Days NLP Challenge

Google I/O samples


https://github.com/GoogleCloudPlatform/ai-platform-text-classifier-shap

>> Day 1 & Day 2: Basic NLP and semantic analysis

Intellipaat: Natural Language Processing (NLP) Tutorial | NLP Training
https://www.youtube.com/watch?v=KVxIx8f_VpM

ODSC: Understanding Unstructured Data with Language Models - Alex Peattie
https://www.youtube.com/watch?time_continue=37&v=4fMwu7K3HmQ

>> Day 3: Topic modeling

https://github.com/atulsinghphd/NLP/blob/master/TopicModelingUsingLDA.ipynb

https://github.com/moorissa/nmf_nyt

https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/

https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24
https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333320/

Guided LDA : https://github.com/NThakur20/GuidedLDA

Bhargav Srinivasa Desikan - Topic Modelling with Gensim
https://www.youtube.com/watch?v=KZkLmN1Bzok
https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling_unrun.ipynb

Text Analysis https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/text_analysis_tutorial_unrun.ipynb

https://www.youtube.com/watch?v=ZkAFJwi-G98

Evaluation

https://towardsdatascience.com/metrics-for-evaluating-machine-learning-classification-models-python-example-59b905e079a5

>> Day 4: Predict next word 

https://github.com/seyedsaeidmasoumzadeh/Predict-next-word

https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/

Patrick Harrison: Modern NLP in Python | PyData DC 2016 ( 1:11:00)
https://www.youtube.com/watch?v=6zm9NC9uRkk

https://towardsdatascience.com/building-a-next-word-predictor-in-tensorflow-e7e681d4f03f
https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c

>> Day 5: Word Embeddings - word2Vec, Glove, NGram

Minsuk Heo : Word2Vec (introduce and tensorflow implementation)
https://www.youtube.com/watch?v=64qSgA66P-8
https://github.com/minsuk-heo/python_tutorial/blob/master/data_science/nlp/word2vec_tensorflow.ipynb

https://skymind.ai/wiki/word2vec

https://datascience.stackexchange.com/questions/9785/predicting-a-word-using-word2vec-model

https://www.guru99.com/word-embedding-word2vec.html

https://github.com/tensorflow/docs/blob/master/site/en/tutorials/representation/word2vec.md

Unsupervised sentence representation with deep learning
https://blog.myyellowroad.com/unsupervised-sentence-representation-with-deep-learning-104b90079a93


>> Day 6: LSTM, IF-TDF, Naive Bayes

TODO

>> Day 7: Computer vision - CNN

TODO

https://ahmedbesbes.com/understanding-deep-convolutional-neural-networks-with-a-practical-use-case-in-tensorflow-and-keras.html

https://ahmedbesbes.com/automate-the-diagnosis-of-knee-injuries-with-deep-learning-part-1-an-overview-of-the-mrnet-dataset.html

https://ahmedbesbes.com/automate-the-diagnosis-of-knee-injuries-with-deep-learning-part-2-building-an-acl-tear-classifier.html

https://ahmedbesbes.com/automate-the-diagnosis-of-knee-injuries-with-deep-learning-part-3-interpret-models-predictions.html


>> Day 8: Pretrained models - BERT, ElMo, ULMFit

BERT is a method of pre-training language representations, meaning that we train a general-purpose "language understanding" model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP.

BERT Explained: State of the art language model for NLP
https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
https://github.com/google-research/bert

Embeddings from Language Models(ELMo for short)

This model is pre-trained with a self-supervising task called a bidirectional language model; they show that the representation from this model is powerful and improves the state-of-the-art performance on many tasks such as question-answer activities, natural language inference, semantic role labeling, coreference resolution, named-entity recognition, and sentiment analysis.

A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Text
https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/

https://github.com/IreneZihuiLi/deeplearning/blob/master/ELMo_test.py

https://github.com/keitakurita/Practical_NLP_in_PyTorch/blob/master/allennlp/elmo_text_classification.ipynb

https://gluon-nlp.mxnet.io/examples/sentence_embedding/elmo_sentence_representation.html

Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python
https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/
https://github.com/navneetkrc/Colab_fastai/blob/master/ULMFiT_fastai_Text_Classification.ipynb

The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)
http://jalammar.github.io/illustrated-bert/


>> Day 9: Transfer learning

A neural network is trained on a data. This network gains knowledge from this data, which is compiled as “weights” of the network. These weights can be extracted and then transferred to any other neural network. Instead of training the other neural network from scratch, we “transfer” the learned features.

A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning

https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a

Transfer Learning using ELMO Embeddings
https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c


>> Day 10: Research paper

TODO

>> Aditional references

A Friendly Introduction to Machine Learning
https://www.youtube.com/watch?v=IpGxLWOIZy4

But what is a Neural Network? 
https://www.youtube.com/watch?v=aircAruvnKk&t=5s

Gradient descent, how neural networks learn 
https://www.youtube.com/watch?v=IHZwWFHWa-w&t=59s

What is backpropagation really doing?
https://www.youtube.com/watch?v=Ilg3gGewQ5U

Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)
https://www.youtube.com/watch?v=WCUNPb-5EYI&t=176s


Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models
https://explosion.ai/blog/deep-learning-formula-nlp

ImageNet: VGGNet, ResNet, Inception, and Xception with Keras

ImageNet is formally a project aimed at (manually) labeling and categorizing images into almost 22,000 separate object categories for the purpose of computer vision research.

https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/
https://github.com/fchollet/deep-learning-models



